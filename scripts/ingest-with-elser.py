#!/usr/bin/env python3
"""
Ingest Sample Data with ELSER Embeddings

This script loads sample product data and ingests it into Elasticsearch
with ELSER embeddings automatically generated via an ingest pipeline.

ELSER will generate sparse vector embeddings for semantic search without
requiring external API keys or services.
"""

import sys
import os
import json

# Add parent directory to path for config import
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import config

# ELSER configuration
ELSER_INDEX_NAME = "products-elser-search"
ELSER_PIPELINE_ID = "elser-ingest-pipeline"

def connect_to_elasticsearch():
    """Connect to Elasticsearch."""
    print(f"Connecting to Elasticsearch at {config.ELASTICSEARCH_URL}...")

    if hasattr(config, 'ELASTICSEARCH_API_KEY'):
        es = Elasticsearch(
            config.ELASTICSEARCH_URL,
            api_key=config.ELASTICSEARCH_API_KEY,
            verify_certs=config.ELASTICSEARCH_VERIFY_CERTS,
            request_timeout=60
        )
    else:
        es = Elasticsearch(
            config.ELASTICSEARCH_URL,
            basic_auth=(config.ELASTICSEARCH_USERNAME, config.ELASTICSEARCH_PASSWORD),
            verify_certs=config.ELASTICSEARCH_VERIFY_CERTS,
            request_timeout=60
        )

    if not es.ping():
        print("ERROR: Could not connect to Elasticsearch")
        sys.exit(1)

    print("✓ Connected to Elasticsearch\n")
    return es

def load_sample_data():
    """Load sample product data."""
    data_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        config.SAMPLE_DATA_PATH
    )

    print(f"Loading sample data from: {data_path}")

    if not os.path.exists(data_path):
        print(f"ERROR: Sample data file not found: {data_path}")
        sys.exit(1)

    with open(data_path, 'r') as f:
        data = json.load(f)

    print(f"✓ Loaded {len(data)} products\n")
    return data

def prepare_elser_documents(products):
    """Prepare documents for ELSER indexing."""
    print("Preparing documents for ELSER ingestion...")

    documents = []

    for product in products:
        # Create combined text field for ELSER
        # This combines key information that should be searchable
        elser_text_parts = [
            product.get('name', ''),
            product.get('description', ''),
            product.get('category', ''),
        ]

        # Add features if available
        if 'features' in product and product['features']:
            if isinstance(product['features'], list):
                elser_text_parts.extend(product['features'])
            else:
                elser_text_parts.append(str(product['features']))

        # Add specifications as text
        if 'specifications' in product and product['specifications']:
            specs = product['specifications']
            for key, value in specs.items():
                elser_text_parts.append(f"{key}: {value}")

        # Combine all text
        elser_text = " | ".join(filter(None, elser_text_parts))

        # Create document
        doc = {
            '_index': ELSER_INDEX_NAME,
            '_id': product.get('id'),
            'pipeline': ELSER_PIPELINE_ID,  # Use ELSER ingest pipeline
            '_source': {
                'id': product.get('id'),
                'name': product.get('name'),
                'category': product.get('category'),
                'price': product.get('price'),
                'description': product.get('description'),
                'specifications': product.get('specifications', {}),
                'features': product.get('features', []),
                'reviews': product.get('reviews', {}),
                'elser_text': elser_text  # Text field for ELSER inference
                # Note: elser_embedding will be auto-generated by the pipeline
            }
        }

        documents.append(doc)

    print(f"✓ Prepared {len(documents)} documents\n")
    return documents

def ingest_documents(es, documents):
    """Bulk ingest documents using ELSER pipeline."""
    print(f"Ingesting {len(documents)} documents with ELSER embeddings...")
    print("(This may take a few minutes as ELSER processes each document)\n")

    try:
        # Use bulk helper for efficient ingestion
        success, failed = bulk(
            es,
            documents,
            chunk_size=10,  # Smaller chunks for ELSER processing
            raise_on_error=False,
            raise_on_exception=False,
            max_retries=3,
            initial_backoff=2
        )

        print(f"✓ Successfully indexed: {success} documents")

        if failed:
            print(f"⚠ Failed to index: {len(failed)} documents")
            for item in failed[:5]:  # Show first 5 failures
                print(f"  - {item}")

        # Refresh index
        es.indices.refresh(index=ELSER_INDEX_NAME)
        print(f"✓ Index refreshed\n")

        return success > 0

    except Exception as e:
        print(f"ERROR during ingestion: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_ingestion(es):
    """Verify documents were indexed with ELSER embeddings."""
    print("Verifying ingestion...")

    try:
        # Get index stats
        stats = es.indices.stats(index=ELSER_INDEX_NAME)
        doc_count = stats['_all']['primaries']['docs']['count']

        print(f"✓ Total documents in index: {doc_count}")

        # Get a sample document to verify ELSER embedding
        response = es.search(
            index=ELSER_INDEX_NAME,
            body={
                "size": 1,
                "query": {"match_all": {}}
            }
        )

        if response['hits']['hits']:
            sample_doc = response['hits']['hits'][0]['_source']

            # Check if ELSER embedding exists
            if 'elser_embedding' in sample_doc:
                embedding = sample_doc['elser_embedding']
                print(f"✓ ELSER embeddings generated successfully")
                print(f"  Sample embedding tokens: {len(embedding)}")
                print(f"  Sample product: {sample_doc.get('name', 'Unknown')}\n")
            else:
                print("⚠ WARNING: ELSER embedding field not found in documents")
                print("  This may indicate the ingest pipeline didn't run correctly\n")
        else:
            print("⚠ No documents found in index\n")

    except Exception as e:
        print(f"Error during verification: {e}\n")

def print_search_example():
    """Print example ELSER search query."""
    print("=" * 70)
    print("Sample ELSER Search Query")
    print("=" * 70)
    print(f"""
To search using ELSER, run this query in Kibana Dev Tools:

POST {ELSER_INDEX_NAME}/_search
{{
  "query": {{
    "sparse_vector": {{
      "field": "elser_embedding",
      "inference_id": "elser-inference-endpoint",
      "query": "laptop for programming"
    }}
  }},
  "size": 5
}}

Or try these sample queries:
- "affordable monitor for home office"
- "high performance gaming laptop"
- "wireless keyboard with long battery life"
- "portable storage device with fast transfer speeds"

ELSER automatically understands semantic meaning and context!
""")
    print("=" * 70)

def main():
    """Main ingestion function."""
    print("=" * 70)
    print("ELSER Data Ingestion Script")
    print("=" * 70)
    print()

    try:
        # Connect to Elasticsearch
        es = connect_to_elasticsearch()

        # Check if index exists
        if not es.indices.exists(index=ELSER_INDEX_NAME):
            print(f"ERROR: Index '{ELSER_INDEX_NAME}' does not exist")
            print("Run setup-elser.py first to create the index and pipeline\n")
            sys.exit(1)

        # Check if pipeline exists
        try:
            es.ingest.get_pipeline(id=ELSER_PIPELINE_ID)
        except:
            print(f"ERROR: Ingest pipeline '{ELSER_PIPELINE_ID}' does not exist")
            print("Run setup-elser.py first to create the pipeline\n")
            sys.exit(1)

        # Load sample data
        products = load_sample_data()

        # Prepare documents
        documents = prepare_elser_documents(products)

        # Ingest documents
        if not ingest_documents(es, documents):
            print("ERROR: Ingestion failed")
            sys.exit(1)

        # Verify ingestion
        verify_ingestion(es)

        # Print search example
        print_search_example()

        es.close()

        print("\n✓ Data ingestion complete!")

    except KeyboardInterrupt:
        print("\n\nIngestion interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
