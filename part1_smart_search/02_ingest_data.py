#!/usr/bin/env python3
"""
Part 1 - Step 2: Ingest Sample Data with ELSER Embeddings

This script loads sample product data and ingests it into Elasticsearch.
ELSER embeddings are automatically generated via the ingest pipeline.

No API keys needed - ELSER runs entirely within Elasticsearch!
"""

import sys
import os
import json

# Add parent directory to path for config import
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from tqdm import tqdm
import config

def connect_to_elasticsearch():
    """Connect to Elasticsearch."""
    if hasattr(config, 'ELASTICSEARCH_API_KEY') and config.ELASTICSEARCH_API_KEY:
        es = Elasticsearch(
            config.ELASTICSEARCH_URL,
            api_key=config.ELASTICSEARCH_API_KEY,
            verify_certs=config.ELASTICSEARCH_VERIFY_CERTS,
            request_timeout=60
        )
    else:
        es = Elasticsearch(
            config.ELASTICSEARCH_URL,
            basic_auth=(config.ELASTICSEARCH_USERNAME, config.ELASTICSEARCH_PASSWORD),
            verify_certs=config.ELASTICSEARCH_VERIFY_CERTS,
            request_timeout=60
        )

    if not es.ping():
        print("ERROR: Could not connect to Elasticsearch")
        sys.exit(1)

    return es

def load_sample_data():
    """Load sample product data."""
    data_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        config.SAMPLE_DATA_PATH
    )

    print(f"Loading data from: {data_path}")

    if not os.path.exists(data_path):
        print(f"ERROR: Data file not found: {data_path}")
        sys.exit(1)

    with open(data_path, 'r') as f:
        data = json.load(f)

    print(f"✓ Loaded {len(data)} products\n")
    return data

def prepare_documents(products):
    """Prepare documents for ELSER indexing."""
    print("Preparing documents...")

    documents = []

    for product in tqdm(products, desc="Processing"):
        # Combine text fields for ELSER
        elser_text_parts = [
            product.get('name', ''),
            product.get('description', ''),
            product.get('category', ''),
        ]

        # Add features
        if 'features' in product and product['features']:
            if isinstance(product['features'], list):
                elser_text_parts.extend(product['features'])
            else:
                elser_text_parts.append(str(product['features']))

        # Add specifications
        if 'specifications' in product and product['specifications']:
            for key, value in product['specifications'].items():
                elser_text_parts.append(f"{key}: {value}")

        # Combine all text
        elser_text = " | ".join(filter(None, elser_text_parts))

        # Create document
        doc = {
            '_index': config.INDEX_NAME,
            '_id': product.get('id'),
            'pipeline': config.ELSER_PIPELINE_ID,  # Use ELSER pipeline
            '_source': {
                'id': product.get('id'),
                'name': product.get('name'),
                'category': product.get('category'),
                'price': product.get('price'),
                'description': product.get('description'),
                'specifications': product.get('specifications', {}),
                'features': product.get('features', []),
                'reviews': product.get('reviews', {}),
                'elser_text': elser_text
                # elser_embedding will be auto-generated by pipeline
            }
        }

        documents.append(doc)

    print(f"✓ Prepared {len(documents)} documents\n")
    return documents

def ingest_documents(es, documents):
    """Bulk ingest documents."""
    print(f"Ingesting {len(documents)} documents with ELSER embeddings...")
    print("(ELSER processes each document - this may take a few minutes)\n")

    try:
        success, failed = bulk(
            es,
            documents,
            chunk_size=10,  # Smaller chunks for ELSER processing
            raise_on_error=False,
            raise_on_exception=False,
            max_retries=3,
            initial_backoff=2
        )

        print(f"\n✓ Successfully indexed: {success} documents")

        if failed:
            print(f"⚠ Failed to index: {len(failed)} documents")

        # Refresh index
        es.indices.refresh(index=config.INDEX_NAME)
        print(f"✓ Index refreshed\n")

        return success > 0

    except Exception as e:
        print(f"ERROR during ingestion: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_ingestion(es):
    """Verify documents were indexed with ELSER embeddings."""
    print("Verifying ingestion...")

    # Get index stats
    stats = es.indices.stats(index=config.INDEX_NAME)
    doc_count = stats['_all']['primaries']['docs']['count']
    print(f"✓ Total documents in index: {doc_count}")

    # Get sample document
    response = es.search(
        index=config.INDEX_NAME,
        body={"size": 1, "query": {"match_all": {}}}
    )

    if response['hits']['hits']:
        sample_doc = response['hits']['hits'][0]['_source']

        if 'elser_embedding' in sample_doc:
            embedding = sample_doc['elser_embedding']
            print(f"✓ ELSER embeddings generated")
            print(f"  Sample embedding tokens: {len(embedding)}")
            print(f"  Sample product: {sample_doc.get('name', 'Unknown')}\n")
        else:
            print("⚠ WARNING: ELSER embedding field not found\n")
    else:
        print("⚠ No documents found in index\n")

def main():
    """Main ingestion function."""
    print("=" * 70)
    print("Part 1 - Step 2: Data Ingestion with ELSER")
    print("=" * 70)
    print()

    try:
        # Connect
        print(f"Connecting to Elasticsearch at {config.ELASTICSEARCH_URL}...")
        es = connect_to_elasticsearch()
        print("✓ Connected\n")

        # Check if index exists
        if not es.indices.exists(index=config.INDEX_NAME):
            print(f"ERROR: Index '{config.INDEX_NAME}' does not exist")
            print("Run 01_setup_index.py first\n")
            sys.exit(1)

        # Check if pipeline exists
        try:
            es.ingest.get_pipeline(id=config.ELSER_PIPELINE_ID)
        except:
            print(f"ERROR: Ingest pipeline '{config.ELSER_PIPELINE_ID}' does not exist")
            print("Run 01_setup_index.py first\n")
            sys.exit(1)

        # Load data
        products = load_sample_data()

        # Prepare documents
        documents = prepare_documents(products)

        # Ingest
        if not ingest_documents(es, documents):
            print("ERROR: Ingestion failed")
            sys.exit(1)

        # Verify
        verify_ingestion(es)

        print("=" * 70)
        print("✓ Data Ingestion Complete!")
        print("=" * 70)
        print("\nNext step: Run 03_semantic_search.py to test search")
        print()

        es.close()

    except KeyboardInterrupt:
        print("\n\nIngestion interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\nERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
